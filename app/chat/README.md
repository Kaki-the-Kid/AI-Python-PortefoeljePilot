# Chat med din CV-assistent

## Arkitektur: Lokal AI-assistent med JSON som kontekst

### 1. Promptmotor

- L칝ser `competence.json`
- Kombinerer det med brugerens sp칮rgsm친l
- Sender det til en AI-model (lokal eller API)

### 2. Flask-route: `/chat`

- POST-endpoint der modtager sp칮rgsm친l
- Returnerer AI-genereret svar
- Kan udvides med Material UI eller WebSocket

### 3. **Eksempel p친 promptmotor**

```python
def generate_cv_response(user_input, competence_data):
    prompt = f"""
Du er en professionel CV-assistent. Brug f칮lgende JSON-data som baggrund:

{json.dumps(competence_data, indent=2, ensure_ascii=False)}

Brugerens sp칮rgsm친l: {user_input}

Svar med en professionel, dansk tekst.
"""
    response = call_ai_model(prompt)  # fx OpenAI, LM Studio, LMDeploy
    return response
```

---

## Flask-chatroute

```python
@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("message")
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        competence_data = json.load(f)
    reply = generate_cv_response(user_input, competence_data)
    return jsonify({"reply": reply})
```

---

## 游눫 Frontend: Material 3 chat UI (valgfrit)

Du kan bruge:

- `md-outlined-text-field` til input
- `md-elevated-card` til svar
- `fetch("/chat", { method: "POST", body: JSON.stringify({ message }) })` i JS

---

## Hvis du vil bruge det fra ekstern webside

- Ekspon칠r `/chat` via reverse proxy (fx `https://pilot.karsten.dk/chat`)
- Din eksterne webside kan sende POST-requests og vise svaret

---

## N칝ste trin

1. Vil du bruge en lokal AI-model (fx LM Studio) eller en API som OpenAI?
2. Skal vi bygge en Material UI-chat i browseren?
3. Skal den kunne foresl친 tekst, rette formuleringer eller generere hele ans칮gninger?

## Min AI-co-pilot

Klar til at skrive min f칮rste promptmotor
